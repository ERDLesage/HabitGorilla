---
title: "Explore_LG_thesis_data"
author: "ERDLesage"
date: "07/02/2022"
output: 
  html_document: 
    highlight: textmate
    theme: spacelab
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pracma)   #for tic, toc
library(Rmisc) # for summarySE
library(tidyverse)
library(gtsummary) # for nice tables
library(ggvenn) # for a venn diagram
library(afex) # for analyses
library(phia) # for analyses - interactions
library(broom.mixed)
source("C:/Users/elise.000/OneDrive/Documents/r_scripts/gorilla_scripts/HabitGorilla/SageThemesNSchemes.R")
source("C:/Users/elise.000/OneDrive/Documents/r_scripts/gorilla_scripts/HabitGorilla/SageDataCleanUtils.R")

D_ <- read.table("C:/Users/elise.000/Documents/AAA_projects/Lisa_data/LG_overtraining.txt", header = TRUE, sep = "\t")
```
# Lisa's overtraining data

## A word of intro
A lot of things went wrong with this dataset. Notably, the counterbalancing. As a result, our hypotheses cannot readily be tested, but perhaps a fair amount of "pilot"-ish hints can be derived from the data.

The data has been preprocessed through the **LGthesis_QA_and_preprocessing.R** script. A couple of filters were created. These need to now be used to continue with the filtered data.

## What proportion of the data is problematic + exclusions

A number of non-recoverable blocks (fubar) should be excluded from analysis. Moreover, blocks with problematic behavior (>20 consecutive trials same respnse, alternating response, or no response; Slackerblock) should be excluded. Persons for which >33% of blocks are slackerblocks should be excluded (N=1 (S17), already excluded). 
Then, based on performance a number of exclusions can be made. That is, it is not reasonable to perform at chance level on day3.  
**Two criteria are used**: 
1. On Days 2 and 3, the average performance across the blocks (per context) must not be below 75%. All data from participants who score less than 75% on a day-level is discarded  
2. Per block, a score of <66% is considered bad performance on days 2 and 3. these participants are not excluded, but the blocks are, for some analyses.
At first glance, this might seem strict/biasing, but looking at the distribution of the scores, it is clearly bimodal with one peak around chance level, suggesting inattention over "genuine" behavior.  

```{r filter the data & summarise, echo=FALSE}
D_ %>% select(Day, fubar, Slackerblock, DayUnder75, BlockUnder66) %>% tbl_summary(by=Day) %>% add_n() %>% # add column with total number of non-missing observations
  bold_labels() 
# filtering out the bad blocks (fubar) and the blocks with "slackerblocks"
D <- D_ %>% filter(fubar==0, Slackerblock==0)
# Identifying and excluding bad performers
ggvenn(D, columns = c("DayUnder75", "BlockUnder66"), stroke_size = 0.5)

AccInd <- D %>% group_by(Subject, Day, StabilityContext) %>% summarySE(measurevar = "OptimalChoice", groupvars = c("Subject", "Day", "StabilityContext"), na.rm=TRUE)
ggplot(AccInd, aes(x=OptimalChoice, Group=Subject)) +   
  geom_histogram(aes(group=Subject, fill=as.factor(Day)), binwidth = 0.05) + 
  ggtitle("Overall accuracy per Day | separate per context")+
  facet_grid(StabilityContext~Day) + 
  geom_vline(xintercept=.75)+
  theme_sage_simple()

# exclude all participants who behave under75 on days 2 or 3
BadPerformers <- D %>% group_by(Subject) %>% summarise(bp=mean(DayUnder75)) %>% mutate(BadPerf = ifelse(bp>0.33, 1, 0))
disp(sprintf("Under75 Exclusions: %d out of %d subjects excluded, %0.2f percent", sum(BadPerformers$BadPerf), nrow(BadPerformers), (sum(BadPerformers$BadPerf)/nrow(BadPerformers))*100))
# filter those guys out
D <- SageExcludeHO(D, BadPerformers, groupvars = c("Subject"))
D <- SageExcludeSimple(D, D$BlockUnder66)
disp(sprintf("Total exclusions: %d / %d datapoints, %0.2f percent of data.\n", nrow(D_)-nrow(D), nrow(D), ((nrow(D_)-nrow(D))/nrow(D))*100))   

rm(D_, BadPerformers, AccInd)
```

## Data overview
```{r show summaries, echo=FALSE}
D %>% select(Day, OptimalChoice, RT) %>% tbl_summary(by=Day, statistic = list(all_continuous() ~ "{mean} ({sd})", all_categorical() ~ "{p}% ({n} / {N})")) %>% add_overall() %>% # add an overall column
  add_n() %>% # add column with total number of non-missing observations
  add_p() %>% # test for a difference between groups
  bold_labels() 
D %>% select(Subject, Day) %>% tbl_summary(by=Day) %>% 
  add_n() %>% # add column with total number of non-missing observations
  bold_labels() 
```

## How do people learn over time? 

```{r spaghetti learning, echo=FALSE}
AccGrp <- D %>% group_by(Day, Block, StabilityContext) %>% summarySE(measurevar = "OptimalChoice", groupvars = c("Block", "StabilityContext"), na.rm=TRUE)
AccInd <- D %>% group_by(Subject, Day, Block,StabilityContext) %>% summarySE(measurevar = "OptimalChoice", groupvars = c("Subject", "Block", "StabilityContext"), na.rm=TRUE)

RTGrp <- D %>% group_by(Block, StabilityContext) %>% summarySE(measurevar = "RT", groupvars = c("Block", "StabilityContext"), na.rm=TRUE)
RTInd <- D %>% group_by(Subject, Block,StabilityContext) %>% summarySE(measurevar = "RT", groupvars = c("Subject", "Block", "StabilityContext"), na.rm=TRUE) 
# accuracy plot
AccSpaghetti <- ggplot() +   
  geom_line(data=AccInd, aes(x=Block, y=OptimalChoice, group=Subject, color=Subject), size =   0.1, alpha=.5) +
  geom_line(data=AccGrp, aes(x=Block, y=OptimalChoice), size=1)+
  facet_grid(StabilityContext~.)+
  xlab("Overtraining") + ylab("Accuracy")+ggtitle("Accuracy over time")+
  theme_sage_simple() 
AccSpaghetti
# RT plot
RTspaghetti <- ggplot() +   
  geom_line(data=RTInd, aes(x=Block, y=RT, group=Subject, color=Subject), size =   0.1, alpha=.5) +
  geom_line(data=RTGrp, aes(x=Block, y=RT), size=1)+
  facet_grid(StabilityContext~.)+
  xlab("Overtraining") + ylab("RT")+ggtitle("RT over time")+
  theme_sage_simple() 
RTspaghetti

rm(RTGrp, RTInd, AccGrp, AccInd, AccSpaghetti, RTspaghetti)
```
## Statistics learning

### mixed model anova
**Fixed factors** : Day, StabilityContext
**Random factors** : Subject
**Dependent variables** : RT and accuracy

```{r mixed models, echo=FALSE}
mAcc <- glmer(OptimalChoice~Day*StabilityContext+(1|Subject), data=D, family="binomial")
anova(mAcc)
summary(mAcc)
tbl_regression(mAcc, exponentiate=TRUE)

mRT <- lmer(RT~Day*StabilityContext+(1|Subject), data=D)
anova(mRT)
tbl_regression(mRT)

```


## Can we see outcome-insensitivity develop throughout overtraining? i.e. Do ppl deal differently with the "exception" over time, and/or depending on context?

Correct choices are 80% reinforced. We might expect participants to "learn" from the misleading outcome ("exception") more (a) in the beginning of the expt and (b) in the volatile conditions. This is effectively a form of outcome insensitivity.  
We center the blocks, because 1->16*3

### We wonder:
1. is performance worse 1 trial after an exception?
2. 

```{r micro outcome insensitivity}
# center block
D$BlockCentered <- scale(D$Block, center=TRUE, scale=TRUE)

# on accuracy
oiAccS <- glmer(OptimalChoice~ExceptionLag*BlockCentered+(1|Subject), data=filter(D, StabilityContext=="StabielC"), family=binomial)
oiAccV <- glmer(OptimalChoice~ExceptionLag*BlockCentered+(1|Subject), data=filter(D, StabilityContext=="OnstabielC"), family=binomial)
oiAcc <- glmer(OptimalChoice~ExceptionLag*StabilityContext*BlockCentered+(1|Subject), data=D, family=binomial)
oiAccSH <- glmer(OptimalChoice~ExceptionLag*SwitchHistory+(1|Subject), data=D, family=binomial)

# on RT
oiRT <- lmer(RT~ExceptionLag*BlockCentered+(1|Subject), data=D)
oiRTSH <- lmer(RT~ExceptionLag*SwitchHistory+(1|Subject), data=D)
oiRTBlockContext <- lmer(RT~ExceptionLag*BlockCentered*StabilityContext+(1|Subject), data=D)

anova(oiAcc)
tbl_regression(oiAcc)
anova(oiAccSH)
tbl_regression(oiAccSH)

anova(oiRT)
tbl_regression(oiRT)
anova(oiRTSH)
tbl_regression(oiRTSH)
anova(oiRTBlockContext)
tbl_regression(oiRTBlockContext)

```

## Micro-habits? The effect of the switch history

Rather 


## Now a little more difficult
